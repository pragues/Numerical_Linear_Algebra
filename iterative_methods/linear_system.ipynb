{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6806d8f6",
   "metadata": {},
   "source": [
    "### Iterative method for solving linear system\n",
    "\n",
    "Direct method for solving a linear system $Ax=b$ by Gaussian elimination has a time complexity of $O(N^3)$. Compring to this exact solution after a finite number of steps, we try to use iterative methods that produce a sequence of approximations $x^{(k)} \\rightarrow x$ with the advantages of:\n",
    "\n",
    "- less memory\n",
    "- faster computation\n",
    "- may handle sparsity more easily\n",
    "\n",
    "In general, 2 classes of iterative methods:\n",
    "\n",
    "- stationary methods\n",
    "- Krylov subspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a03a5f",
   "metadata": {},
   "source": [
    "\n",
    "## stationary methods\n",
    "\n",
    "Finds a splitting: $A=A_1+A_2$ \n",
    "\n",
    "$Ax=b \\Leftrightarrow A_1 x = -A_2 x+b \\therefore x = - A_1^{-1} A_2 x+A_1^{-1} b $\n",
    "\n",
    "ç„¶åå°†ç­‰å¼å·¦è¾¹çœ‹åšæ–°ä¸€ä¸ªæœªçŸ¥æ•°æ¥æ„å»ºiteration: $x^{(k+1)} = - A_1^{-1} (A_2 x^{(k)}+ b) = M x^{(k)} + C$\n",
    "\n",
    "å¦‚æœ$A_2$æ˜¯0ï¼Œé‚£ä¸€æ­¥å°±å¯ä»¥è§£å‡ºæ¥ï¼›å¦‚æœ$A_2$å¾ˆå°ï¼Œæ­¤æ—¶$A_1$å·²ç»å¾ˆæ¥è¿‘$A$ï¼Œå¸¦ç²—ç³™è§£å¸¦å…¥è¢«å¿½ç•¥çš„$A_2$éƒ¨åˆ†å»è®¡ç®—æ®‹å·®ï¼Œç”¨è¿™ä¸ªè¯¯å·®å»ä¿®æ­£ä¸‹ä¸€æ¬¡çš„è§£ã€‚\n",
    "\n",
    "iterative methods that uses this idea: Jacobi method, Gauss-Seidel method\n",
    "\n",
    "ä¸æ˜¯å¯¹æ‰€æœ‰çš„$A$éƒ½ç®¡ç”¨ï¼Œ$x^{(k)}$èƒ½å¤Ÿæ”¶æ•›åˆ°çœŸå®è§£ $x^*$ çš„å‰æï¼šspectral radius (which is $|\\lambda_{\\max}|$) of $M=-A_1^{-1}A_2$ å°äº1 $\\rho(-A_1^{-1}A_2) <1$ ç†ç”±å¤§æ¦‚æ˜¯æˆ‘ä»¬å¸Œæœ›æ¯æ¬¡è¿™ä¸ªè¯¯å·®æœ€åæ˜¯convergentçš„ï¼šlet $e_k = x_k-x*, e_k = M^{k} \\cdot e_0, ||M^k|| \\rightarrow 0 $ iff $\\rho(M)<1$\n",
    "\n",
    "è¿™ä¸ªæ–¹æ³•ä¼˜åŒ–äº†å•¥å‘¢ï¼Ÿå°¤å…¶åœ¨$A$æ˜¯sparse matrixçš„æ—¶å€™ï¼Œæ­¤æ—¶å¯¹$A$ è¿›è¡ŒGaussian EliminationåŸæœ¬0çš„ä½ç½®å˜æˆé0çš„æ•°(fill-in)ç ´åäº†æœ¬æ¥èƒ½å¤Ÿç”¨æ³¨å…¥linked-listæ¥çœå†…å­˜çš„sparsityç»“æ„ã€‚ä»¥åŠçŸ¥é“$M=-A_1^{-1}A_2$ä¹‹åï¼Œåšçš„æ˜¯matrix-vectorä¹˜æ³•ã€‚\n",
    "\n",
    "* Let $\\|\\cdot\\|$ be an induced matrix norm, then $\\rho(M) \\leq \\|M\\|$. For any matrix $M$ and $\\epsilon > 0$, there's an induced norm $\\|\\cdot\\|$ such that $\\|M\\| \\leq \\rho(M) + \\epsilon$\n",
    "\n",
    "* **Proof.** Let $Mx = \\lambda x$ and $|\\lambda| = \\rho(M)$, then $\\|M\\| = \\max_{y \\neq 0} \\frac{\\|My\\|}{\\|y\\|} \\geq \\frac{\\|Mx\\|}{\\|x\\|} = |\\lambda| = \\rho(M)$\n",
    "\n",
    "* For the second part, let the Jordan form of $M$ be $S^{-1}MS = J$. Let $D_{\\epsilon} = \\text{diag}(1, \\epsilon, \\epsilon^2, \\ldots, \\epsilon^{m-1})$\n",
    "Then $(SD_{\\epsilon})^{-1}M(SD_{\\epsilon}) = D_{\\epsilon}^{-1}JD_{\\epsilon}$\n",
    "\n",
    "Define a new vector norm $\\|x\\| = \\|(SD_{\\epsilon})^{-1}x\\|_{\\infty}$\n",
    "\n",
    "The corresponding induced norm\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|M\\| &= \\max_{y \\neq 0} \\frac{\\|My\\|}{\\|y\\|} = \\max_{y \\neq 0} \\frac{\\|(SD_{\\epsilon})^{-1}My\\|_{\\infty}}{\\|(SD_{\\epsilon})^{-1}y\\|_{\\infty}} \\\\\n",
    "&= \\max_{x \\neq 0} \\frac{\\|(SD_{\\epsilon})^{-1}M(SD_{\\epsilon})x\\|_{\\infty}}{\\|x\\|_{\\infty}} = \\|(SD_{\\epsilon})^{-1}M(SD_{\\epsilon})\\|_{\\infty} \\\\\n",
    "&= \\|D_{\\epsilon}^{-1}JD_{\\epsilon}\\|_{\\infty} = \\max |\\lambda_i| + \\epsilon = \\rho(M) + \\epsilon\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76069b30",
   "metadata": {},
   "source": [
    "#### Jacobi Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cac6f3",
   "metadata": {},
   "source": [
    "####  Gauss-Seidel Iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84897ee8",
   "metadata": {},
   "source": [
    "## Krylov subspace methods\n",
    "\n",
    "Uses only matrix-vector multiplication $Au$\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/567939767\n",
    "\n",
    "small proof of Cayley-Hamilton theory: https://math.stanford.edu/~eliash/Public/53h-2011/brendle.pdf\n",
    "\n",
    "é¦–å…ˆå•¥æ˜¯Krylov subspace?\n",
    "\n",
    "å¯¹äºçŸ©é˜µ$A$ã€‚å…¶characteristics polynomialæ˜¯ä¸€ä¸ªé›¶åŒ–å¤šé¡¹å¼ï¼Œå³å­˜åœ¨ä¸€ç»„ç³»æ•°ä½¿å¾—$f(A)=A^n+a_{n-1}A^{n-1}+...+a_0 I=0 \\Leftrightarrow A^{-1} = -\\frac{1}{a_0}A^{n-1}-\\frac{a_{n-1}}{a_0}A^{n-2}-...-\\frac{a_1}{a_0}I (ğŸ¥°)$\n",
    "\n",
    "åœ¨æ±‚è§£$Ax=b$çš„æ—¶å€™ï¼Œä»»æ„ç»™å®šä¸€ä¸ªåˆå§‹å€¼$x_0$, å¯¹åº”äº§ç”Ÿçš„åˆå§‹æ®‹å·®$r_0=b-Ax_0$åˆ™è¯¥æ–¹ç¨‹ç»„çš„ç²¾ç¡®è§£å¯ä»¥è¡¨ç¤ºä¸º$x=x_0+A^{-1}r_0 = x_0 -\\frac{1}{a_0}A^{n-1} r_0-\\frac{a_{n-1}}{a_0}A^{n-2}r_0-...-\\frac{a_1}{a_0}r_0$ \n",
    "\n",
    "å› æ­¤ç†è®ºä¸Šå¯ä»¥åœ¨ç©ºé—´$\\{r_0, Ar_0, ..., A^{n-1} r_0\\}$ä¸­æ‰¾åˆ°æ–¹ç¨‹çš„ç²¾ç¡®è§£ã€‚ä½†æ˜¯åœ¨å®é™…çš„è®¡ç®—ä¸­$n$çš„æ•°é‡çº§å¯èƒ½å¾ˆå¤§ï¼Œå¯¹äºç²¾ç¡®è§£çš„ä»£ä»·ä¹Ÿä¼šå¾ˆé«˜ï¼Œå¯„å¸Œæœ›ä¸åœ¨è¿™ä¸ªç©ºé—´çš„ä¸€ä¸ªä½ç»´åº¦å­ç©ºé—´ä¸­æœç´¢è¿‘ä¼¼è§£ã€‚\n",
    "\n",
    "Then we give a definition of Krylov subspace of $m$-dim: $\\kappa_m = \\text{span}(r, Ar, A^2 r, ..., A^{m-1} r)$. Suppose $r, Ar, A^2 r, ..., A^{m-1} r$ linearly independent $\\Leftrightarrow \\kappa_m=m$. $v_1, v_2, ..., v_m$ a basis of $\\kappa_m$. Then any vector in $\\kappa_m$ can be represented as $x = V_m y$ where $V_m=[v_1, ..., v_m]$, $y$ the corresponding scalar. \n",
    "\n",
    "å‡è®¾è§£æè§£(closed form representation)åœ¨ä»¿å°„ç©ºé—´$x_0+\\kappa_m$ä¸­çš„æœ€ä½³è¿‘ä¼¼è§£ä¸º$x^m$, æˆ‘ä»¬æœ‰$x_m = x_0+t, t \\in \\kappa_m$. å› æ­¤å¯¹äº$x^m$çš„æ±‚è§£å¯ä»¥è½¬åŒ–ä¸ºï¼š 1. å¯»æ‰¾ä¸€ç»„åˆé€‚çš„basis, 2.æ±‚å‡º$x^m$åœ¨è¿™ç»„basisä¸‹çš„çº¿æ€§è¡¨å‡ºç³»æ•°$y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863bc77",
   "metadata": {},
   "source": [
    "#### Arnoldi Iteration\n",
    "\n",
    "a modified Gram-Schimidt-style iteration for transforming a matrix into Hessenberg form.\n",
    "\n",
    "Gram-Schimidtæ­£äº¤åŒ–ä¹‹åçš„basis\n",
    "\n",
    "When $||r_m||_2 = ||b-Ax^{m}||_2$ reaches its min , we must have $b-Ax^{m} \\perp A \\kappa_m$, and $A \\kappa_m = \\text{span} \\{Ar, A^2 r, ..., A^m r\\} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecde55a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " [[ 0.4472136   0.89442719  0.        ]\n",
      " [ 0.89442719 -0.4472136   0.        ]]\n",
      "H:\n",
      " [[4. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# arnoldi from scratch: iterate k times?\n",
    "import numpy as np\n",
    "\n",
    "def arnoldi(A, b, k):\n",
    "    n = len(b)\n",
    "    Q = np.zeros((n, k+1))\n",
    "    H = np.zeros((k+1, k))\n",
    "\n",
    "    Q[:, 0] = b / np.linalg.norm(b)\n",
    "\n",
    "    for j in range(k):\n",
    "        v = A @ Q[:, j]\n",
    "\n",
    "        # gram-schmidt\n",
    "        for i in range(j+1):\n",
    "            H[i, j] = np.dot(Q[:, i], v)\n",
    "            v = v - H[i, j] * Q[:, i]\n",
    "        \n",
    "        H[j+1, j] = np.linalg.norm(v)\n",
    "        if H[j+1, j] == 0:\n",
    "            return Q, H\n",
    "        Q[:, j+1] = v / H[j+1, j]\n",
    "\n",
    "        return Q, H\n",
    "\n",
    "A = np.array([[4., 1.], [1., 3.]])\n",
    "b = np.array([1., 2.])\n",
    "\n",
    "Q, H = arnoldi(A, b, k=2)\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"H:\\n\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2a3f1",
   "metadata": {},
   "source": [
    "#### GMRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb007d31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "è¿™æ˜¯å›¾ç‰‡å†…å®¹çš„ Markdown ç‰ˆæœ¬ï¼Œå…¬å¼å·²è½¬æ¢ä¸º LaTeX æ ¼å¼ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨ï¼š\n",
    "\n",
    "å¯¹äºä»»æ„å‘é‡ $x \\in x_0 + \\kappa_m$ ï¼Œå¯è®¾ $x = x_0 + V_m y$ ï¼Œåˆ™ï¼š\n",
    "\n",
    "$r = b - Ax$\n",
    "\n",
    "$r = b - A(x_0 + V_m y) $\n",
    "\n",
    "$r = r_0 - A V_m y $\n",
    "\n",
    "$r = \\beta v_1 - V_{m+1} H_{m+1,m} y$\n",
    "\n",
    "$r = V_{m+1} (\\beta e_1 - H_{m+1,m} y)$\n",
    "\n",
    "ç”±äº $V_{m+1}$ åˆ—æ­£äº¤ï¼Œæ‰€ä»¥ $||r||_2 = ||V_{m+1} (\\beta e_1 - H_{m+1,m} y)||_2 = ||\\beta e_1 - H_{m+1,m} y||_2$\n",
    "\n",
    "è¯æ˜ï¼šåˆ—æ­£äº¤çŸ©é˜µä¸€å®šæ˜¯è¡Œæ­£äº¤çš„ï¼Œå³ $V_{m+1}$ æ˜¯æ­£äº¤çŸ©é˜µï¼Œå¯¹äºæ­£äº¤çŸ©é˜µ $A$ è€Œè¨€ï¼š\n",
    "$||Ax||_2 = ||x||_2$ ï¼Œå› ä¸º\n",
    "\n",
    "$||Ax||_2 = (Ax)^T (Ax) = x^T A^T A x = x^T I x = x^T x = ||x||_2$\n",
    "\n",
    "å½“ä½¿å¾—æ®‹é‡æœ€å°æ—¶ï¼Œä¸Šè¿°é—®é¢˜å°±æ˜¯ä¸€ä¸ªçº¿æ€§æœ€å°äºŒä¹˜é—®é¢˜\n",
    "\n",
    "éšç€mçš„å¢å¤§ï¼Œå¯èƒ½ä¼šæœ‰$r, Ar, ..., A^{m-1}r$çº¿æ€§æ— å…³çš„æƒ…å†µä½†æ˜¯$A^mr$æ€»å¯ä»¥ç”±çº¿æ€§ç»„åˆè¡¨ç¤ºï¼Œæ­¤æ—¶Arnoldiè¿‡ç¨‹ä¼šæœ‰$h_{m+1, m}=0$ and also $||r||_2 = ||\\beta e_1 -H_m y||_2$ å½“$||r_2||$æœ€å°çš„æ—¶å€™ï¼Œ$y=H_m^{-1}(\\beta e_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d16d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd637321",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colpali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
